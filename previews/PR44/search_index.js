var documenterSearchIndex = {"docs":
[{"location":"setting_up_scalar/#Setting-up-a-Scalar-Random-Feature-Method","page":"Scalar method","title":"Setting up a Scalar Random Feature Method","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"A basic creation of sigmoid-based scalar-valued random feature method is given as follows","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"# user inputs required:\n# paired input-output data - io_pairs::PairedDataContainer \n# parameter distribution   - pd::ParameterDistribution \n# number of features       - n_features::Int\n\nfeature_sampler = FeatureSampler(pd) \nsigmoid_sf = ScalarFeature(n_features, feature_sampler, Sigmoid()) \nrfm = RandomFeatureMethod(sigmoid_sf)\nfitted_features = fit(rfm, io_pairs)","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"Prediction at new inputs are made with","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"# user inputs required\n# new test inputs - i_test::DataContainer\n\npredicted_mean, predicted_var = predict(rfm, fitted_features, i_test)","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"We see the core objects","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"ParameterDistribution: a flexible container for constructing constrained parameter distributions, (from EnsembleKalmanProcesses.jl)\n(Paired)DataContainer: consistent storage objects for input-output pairs or just inputs, (from EnsembleKalmanProcesses.jl)\nFeatureSampler: Builds the random feature distributions from a parameter distribution\nScalarFeature: Builds a feature from the random feature distributions\nRandomFeatureMethod: Sets up the learning problem (with e.g. batching, regularization)\nFit: Stores fitted features from the fit method","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"note: See some examples!\nRunning the test suite with TEST_PLOT_FLAG = true will produce some 1-Dto 1-D and d-D to 1-D example fits produced by test/Methods/runtests.jl. These use realistic optional arguments and distributions.","category":"page"},{"location":"setting_up_scalar/#ParameterDistributions","page":"Scalar method","title":"ParameterDistributions","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The simplest construction of parameter distributions is by using the constrained_gaussian construction.","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"using RandomFeatures.ParameterDistributions","category":"page"},{"location":"setting_up_scalar/#**Recommended**-univariate-and-product-distribution-build","page":"Scalar method","title":"Recommended univariate and product distribution build","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The easiest constructors are for univariate and products","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"# constrained_gaussian(\"xi\", desired_mean, desired_std, lower_bound, upper_bound)\none_dim_pd = constrained_gaussian(\"xi\", 10, 5, -Inf, Inf) # Normal distribution\nfive_dim_pd = constrained_gaussian(\"xi\", 10, 5, 0, Inf, repeats = 5) # Log-normal (approx mean 10 & approx std 5) in each of the five dimensions","category":"page"},{"location":"setting_up_scalar/#Simple-multivariate-distribution","page":"Scalar method","title":"Simple multivariate distribution","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"Simple unconstrained distribution is created as follows. ","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"using Distributions\n\nμ = zeros(3)\nΣ = SymTridiagonal(2 * ones(3), ones(2))\nthree_dim_pd = ParameterDistribution(\n    Dict(\"distribution\" => Parameterized(MvNormal(μ,Σ)), # the distribution\n         \"constraint\" => repeat([no_constraint()],3), # constraints \n         \"name\" => \"xi\",\n      ),\n)","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"note:  xi? \nThe name of the distribution of the features must be \"xi\"","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"note: Further distributions\nCombined distributions can be made using the VectorOfParameterized, or histogram-based distributions with Samples. Extensive documentation of distributions and constraints is found here.","category":"page"},{"location":"setting_up_scalar/#Sampler","page":"Scalar method","title":"Sampler","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The random feature distribution mathcalD is built of two distributions, the user-provided mathcalD_xi (\"xi\") and a bias distribution (\"bias\"). The bias distribution is one-dimensional, and commonly uniformly distributed, so we provide an additional constructor for this case","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"theta = (xib) sim mathcalD = (mathcalD_xi mathcalU(c_ellc_u))","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"Defaults c_ell = 0 c_u = 2pi. In the code this is built as","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"sampler = FeatureSampler(\n    parameter_distribution;\n    uniform_shift_bounds = [0, 2 * π], \n    rng = Random.GLOBAL_RNG\n)","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"A random number generator can be provided. The second argument can be replaced with a general 1-D ParameterDistribution with a name-field \"bias\".","category":"page"},{"location":"setting_up_scalar/#Features:-ScalarFeature-d-D-\\to-1-D","page":"Scalar method","title":"Features: ScalarFeature d-D to 1-D","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"Given xinmathbbR^n input data, and m (n_features) features, Features produces samples of","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"Phi(xtheta_j) = sigma f(xi_jcdot x + b_j)qquad theta_j=(xi_jb_j) sim mathcalDqquad mathrmfor j=1dotsm ","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"Note that Phi in mathbbR^nmp. Choosingf as a cosine to produce fourier features","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"sf = ScalarFourierFeature(\n    n_features,\n    sampler;\n    kwargs...\n) ","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"f as a neuron activation produces a neuron feature (ScalarActivation listed here) ","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"sf = ScalarNeuronFeature(\n    n_features,\n    sampler;\n    activation_fun = Relu(),\n    kwargs...\n) ","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The keyword feature_parameters = Dict(\"sigma\" => a), can be included to set the value of sigma.","category":"page"},{"location":"setting_up_scalar/#Method","page":"Scalar method","title":"Method","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The RandomFeatureMethod sets up the training problem to learn coefficients betainmathbbR^m from input-output training data (xy)=(x_iy_i)_i=1^n, y_i in mathbbR and parameters theta = theta_j_j=1^m:","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"(frac1mPhi^T(xtheta) Phi(xtheta) + lambda I) beta = Phi^T(xtheta)y","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"Where lambda I is a regularization.","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"rfm = RandomFeatureMethod(\n    sf;\n    regularization = 1e12 * eps() * I,\n    batch_sizes = (\"train\" => 0, \"test\" => 0, \"feature\" => 0),\n)","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"One can select batch sizes to balance the space-time (memory-process) trade-off. when building and solving equations by setting values of \"train\", test data \"test\" and number of features \"feature\". The default is no batching (0).","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"warning: Conditioning\nThe problem is ill-conditioned without regularization. If you encounters a Singular or Positive-definite exceptions, try increasing regularization","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The solve for beta occurs in the fit method. In the Fit object we store the system matrix, its factorization, and the inverse of the factorization. For many applications this is most efficient representation, as predictions (particularly of the covariance) are then only a matrix multiplication.","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"fitted_features = fit(\n    rfm,\n    io_pairs; # (x,y)\n    decomposition = \"cholesky\",\n)","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The decomposition is based off the LinearAlgebra.Factorize functions. For performance we have implemented only (\"cholesky\", \"svd\", and for Lambda=0 (not recommended) the method defaults to \"pinv\")","category":"page"},{"location":"setting_up_scalar/#Hyperparameters","page":"Scalar method","title":"Hyperparameters","text":"","category":"section"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"[Coming soon]","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"note: Note\nHyperparameter selection is very important for a good random feature fit.","category":"page"},{"location":"setting_up_scalar/","page":"Scalar method","title":"Scalar method","text":"The hyperparameters are the parameters appearing in the random feature distribution mathcalD. We have an examples where an ensemble-based algorithm is used to optimize such parameters in examples/Learn_hyperparameters/","category":"page"},{"location":"API/Samplers/#Samplers","page":"Samplers","title":"Samplers","text":"","category":"section"},{"location":"API/Samplers/","page":"Samplers","title":"Samplers","text":"CurrentModule = RandomFeatures.Samplers","category":"page"},{"location":"API/Samplers/","page":"Samplers","title":"Samplers","text":"Sampler\nFeatureSampler\nget_parameter_distribution\nget_rng\nsample(rng::AbstractRNG, s::Sampler, n_draws::Int)","category":"page"},{"location":"API/Samplers/#RandomFeatures.Samplers.Sampler","page":"Samplers","title":"RandomFeatures.Samplers.Sampler","text":"struct Sampler{RNG<:Random.AbstractRNG}\n\nWraps the parameter distributions used to sample random features\n\nparameter_distribution::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution: A probability distribution, possibly with constraints\nrng::Random.AbstractRNG: A random number generator state\n\n\n\n\n\n","category":"type"},{"location":"API/Samplers/#RandomFeatures.Samplers.FeatureSampler","page":"Samplers","title":"RandomFeatures.Samplers.FeatureSampler","text":"FeatureSampler(\n    parameter_distribution::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    bias_distribution::Union{Nothing, EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution};\n    rng\n) -> Sampler{Random._GLOBAL_RNG}\n\n\nbasic constructor for a Sampler \n\n\n\n\n\nFeatureSampler(\n    parameter_distribution::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    output_dim::Int64;\n    uniform_shift_bounds,\n    rng\n) -> Sampler{Random._GLOBAL_RNG}\n\n\none can conveniently specify the bias as a uniform-shift uniform_shift_bounds with output_dim dimensions\n\n\n\n\n\n","category":"function"},{"location":"API/Samplers/#RandomFeatures.Samplers.get_parameter_distribution","page":"Samplers","title":"RandomFeatures.Samplers.get_parameter_distribution","text":"get_parameter_distribution(\n    s::Sampler\n) -> EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n\n\ngets the parameter_distribution field \n\n\n\n\n\n","category":"function"},{"location":"API/Samplers/#RandomFeatures.Samplers.get_rng","page":"Samplers","title":"RandomFeatures.Samplers.get_rng","text":"get_rng(s::Sampler) -> Random.AbstractRNG\n\n\ngets the rng field\n\n\n\n\n\n","category":"function"},{"location":"API/Samplers/#StatsBase.sample-Tuple{Random.AbstractRNG, Sampler, Int64}","page":"Samplers","title":"StatsBase.sample","text":"sample(\n    rng::Random.AbstractRNG,\n    s::Sampler,\n    n_draws::Int64\n) -> EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n\n\nsamples the distribution within s, n_draws times using a random number generator rng. Can be called without rng (defaults to s.rng) or n_draws (defaults to 1)\n\n\n\n\n\n","category":"method"},{"location":"API/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"CurrentModule = RandomFeatures.Utilities","category":"page"},{"location":"API/Utilities/#Batching","page":"Utilities","title":"Batching","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"batch_generator","category":"page"},{"location":"API/Utilities/#RandomFeatures.Utilities.batch_generator","page":"Utilities","title":"RandomFeatures.Utilities.batch_generator","text":"batch_generator(\n    array::AbstractArray,\n    batch_size::Int64;\n    dims\n) -> Any\n\n\nproduces batched sub-array views of size batch_size along dimension dims.\n\nnote: Note\nthis creates views not copies. Modifying a batch will modify the original!\n\n\n\n\n\n","category":"function"},{"location":"API/Utilities/#Matrix-Decomposition","page":"Utilities","title":"Matrix Decomposition","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"Decomposition\nStoredInvType\nFactor\nPseInv\nget_decomposition\nget_full_matrix\nget_parametric_type\nlinear_solve","category":"page"},{"location":"API/Utilities/#RandomFeatures.Utilities.Decomposition","page":"Utilities","title":"RandomFeatures.Utilities.Decomposition","text":"struct Decomposition{T, M<:(AbstractMatrix), MorF<:Union{AbstractMatrix, LinearAlgebra.Factorization}}\n\nStores a matrix along with a decomposition T=Factor, or pseudoinverse T=PseInv, and also computes the inverse of the Factored matrix (for several predictions this is actually the most computationally efficient action)\n\nfull_matrix::AbstractMatrix: The original matrix\ndecomposition::Union{AbstractMatrix, LinearAlgebra.Factorization}: The matrix decomposition, or pseudoinverse\ninv_decomposition::AbstractMatrix: The matrix decomposition of the inverse, or pseudoinverse\n\n\n\n\n\n","category":"type"},{"location":"API/Utilities/#RandomFeatures.Utilities.StoredInvType","page":"Utilities","title":"RandomFeatures.Utilities.StoredInvType","text":"abstract type StoredInvType\n\nType used as a flag for the stored Decomposition type\n\n\n\n\n\n","category":"type"},{"location":"API/Utilities/#RandomFeatures.Utilities.Factor","page":"Utilities","title":"RandomFeatures.Utilities.Factor","text":"abstract type Factor <: StoredInvType\n\n\n\n\n\n","category":"type"},{"location":"API/Utilities/#RandomFeatures.Utilities.PseInv","page":"Utilities","title":"RandomFeatures.Utilities.PseInv","text":"abstract type PseInv <: StoredInvType\n\n\n\n\n\n","category":"type"},{"location":"API/Utilities/#RandomFeatures.Utilities.get_decomposition","page":"Utilities","title":"RandomFeatures.Utilities.get_decomposition","text":"get_decomposition(\n    d::Decomposition\n) -> Union{AbstractMatrix, LinearAlgebra.Factorization}\n\n\nget decomposition field\n\n\n\n\n\n","category":"function"},{"location":"API/Utilities/#RandomFeatures.Utilities.get_full_matrix","page":"Utilities","title":"RandomFeatures.Utilities.get_full_matrix","text":"get_full_matrix(d::Decomposition) -> AbstractMatrix\n\n\nget full_matrix field\n\n\n\n\n\n","category":"function"},{"location":"API/Utilities/#RandomFeatures.Utilities.get_parametric_type","page":"Utilities","title":"RandomFeatures.Utilities.get_parametric_type","text":"get_parametric_type(\n    d::Decomposition{T, M<:Union{AbstractMatrix, LinearAlgebra.Factorization}}\n) -> Any\n\n\nget the parametric type\n\n\n\n\n\n","category":"function"},{"location":"API/Utilities/#RandomFeatures.Utilities.linear_solve","page":"Utilities","title":"RandomFeatures.Utilities.linear_solve","text":"linear_solve(\n    d::Decomposition,\n    rhs::AbstractArray{<:AbstractFloat, 3},\n    ::Type{Factor};\n    tullio_threading\n) -> Any\n\n\nSolve the linear system based on Decomposition type\n\n\n\n\n\n","category":"function"},{"location":"parallelism/#Explicit-bottlenecks","page":"Bottlenecks and performance tips","title":"Explicit bottlenecks","text":"","category":"section"},{"location":"parallelism/#Explicit-bottlenecks-2","page":"Bottlenecks and performance tips","title":"Explicit bottlenecks","text":"","category":"section"},{"location":"parallelism/","page":"Bottlenecks and performance tips","title":"Bottlenecks and performance tips","text":"By far the highest computational demand for high-dimensional and/or large data systems is the building of the system matrix, particularly the multiplication Phi^TPhi (for scalar) and Phi_nimPhi_njm. These can be accelerated by multithreading (see below)\nFor large number of features, the inversion inv(factorization(system_matrix)) is noticeable, though typically still small when in the regime of n_features << n_samples * output_dim.\nFor the vector case, the use of non-diagonal Lambda_ij for regularization cannot take advantage of certain matrix-tensor identities and may also cause substantial slow-down. (currently)\nPrediction bottlenecks are largely due to allocations and matrix multiplications. Please see our predict!() methods which allow for users to pass in preallocated of arrays. This is very beneficial for repeated predictions.\nFor systems without enough regularization, positive definiteness may need to be enforced. If done too often, it has non-negligible cost, as it involves calculating eigenvalues of the non p.d. matrix (particularly abs(min(eigval)) that is then added to the diagonal. It is better to add more regularization into Lambda","category":"page"},{"location":"parallelism/#Implicit-bottlenecks","page":"Bottlenecks and performance tips","title":"Implicit bottlenecks","text":"","category":"section"},{"location":"parallelism/","page":"Bottlenecks and performance tips","title":"Bottlenecks and performance tips","text":"The optimization of hyperparameters is a costly operation that may require construction and evaluation of thousands of RandomFeatureMethods. The dimensionality (i.e. complexity) of this task will depend on how many free parameters are taken to be within a distribution though. mathcalO(1000s) parameters may take several hours to optimize (on multiple threads).","category":"page"},{"location":"parallelism/#Parallelism/memory","page":"Bottlenecks and performance tips","title":"Parallelism/memory","text":"","category":"section"},{"location":"parallelism/","page":"Bottlenecks and performance tips","title":"Bottlenecks and performance tips","text":"We make use of Tullio.jl which comes with in-built memory management. We are phasing out our own batches in favour of using this for now.\n[Tullio.jl(https://github.com/mcabbott/Tullio.jl) comes with multithreading routines, Simply call the code with julia --project -t n_threads to take advantage of this. Depending on problem size you may wish to use your own external threading, Tullio will greedily steal threads in this case. To prevent this interference we provide a keyword argument: ","category":"page"},{"location":"parallelism/","page":"Bottlenecks and performance tips","title":"Bottlenecks and performance tips","text":"RandomFeatureMethod(... ; tullio_threading=false) # serial threading during the build and fit! methods\npredict(...; tullio_threading=false) # serial threading for prediction\npredict!(...; tullio_threading=false) # serial threading for in-place prediction","category":"page"},{"location":"parallelism/","page":"Bottlenecks and performance tips","title":"Bottlenecks and performance tips","text":"An example where tullio_threading=false is useful is when optimizing hyperparameters with ensemble methods (see our examples), here one could use threading/multiprocessing approaches across ensemble members to make better use of the embarassingly parallel framework (e.g. see this page for EnsembleKalmanProcessess: Parallelism and HPC. ","category":"page"},{"location":"parallelism/","page":"Bottlenecks and performance tips","title":"Bottlenecks and performance tips","text":"note: Note\nWe do not yet have GPU functionality","category":"page"},{"location":"API/Features/#Features","page":"Features","title":"Features","text":"","category":"section"},{"location":"API/Features/","page":"Features","title":"Features","text":"CurrentModule = RandomFeatures.Features","category":"page"},{"location":"API/Features/","page":"Features","title":"Features","text":"get_scalar_function\nget_feature_sampler\nget_feature_sample\nget_n_features\nget_feature_parameters\nget_output_dim\nsample(rf::RandomFeature)","category":"page"},{"location":"API/Features/#RandomFeatures.Features.get_scalar_function","page":"Features","title":"RandomFeatures.Features.get_scalar_function","text":"get_scalar_function(rf::RandomFeature) -> Any\n\n\ngets the scalar_function field \n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.get_feature_sampler","page":"Features","title":"RandomFeatures.Features.get_feature_sampler","text":"get_feature_sampler(rf::RandomFeature) -> Any\n\n\ngets the feature_sampler field \n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.get_feature_sample","page":"Features","title":"RandomFeatures.Features.get_feature_sample","text":"get_feature_sample(rf::RandomFeature) -> Any\n\n\ngets the feature_sample field \n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.get_n_features","page":"Features","title":"RandomFeatures.Features.get_n_features","text":"get_n_features(rf::RandomFeature) -> Any\n\n\ngets the n_features field \n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.get_feature_parameters","page":"Features","title":"RandomFeatures.Features.get_feature_parameters","text":"get_feature_parameters(rf::RandomFeature) -> Any\n\n\ngets the feature_parameters field \n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.get_output_dim","page":"Features","title":"RandomFeatures.Features.get_output_dim","text":"get_output_dim(rf::ScalarFeature) -> Int64\n\n\ngets the output dimension (equals 1 for scalar-valued features)\n\n\n\n\n\nget_output_dim(rf::VectorFeature) -> Int64\n\n\ngets the output dimension (equals 1 for scalar-valued features)\n\n\n\n\n\n","category":"function"},{"location":"API/Features/#StatsBase.sample-Tuple{RandomFeature}","page":"Features","title":"StatsBase.sample","text":"sample(rf::RandomFeature) -> Any\n\n\nsamples the random feature distribution \n\n\n\n\n\n","category":"method"},{"location":"API/Features/#scalar-features","page":"Features","title":"Scalar Features","text":"","category":"section"},{"location":"API/Features/","page":"Features","title":"Features","text":"ScalarFeature\nScalarFourierFeature\nScalarNeuronFeature\nbuild_features(rf::ScalarFeature,inputs::AbstractMatrix,atch_feature_idx::AbstractVector{Int})","category":"page"},{"location":"API/Features/#RandomFeatures.Features.ScalarFeature","page":"Features","title":"RandomFeatures.Features.ScalarFeature","text":"struct ScalarFeature{S<:AbstractString, SF<:ScalarFunction} <: RandomFeature\n\nContains information to build and sample RandomFeatures mapping from N-D -> 1-D\n\nn_features::Int64: Number of features\nfeature_sampler::Sampler: Sampler of the feature distribution\nscalar_function::ScalarFunction: ScalarFunction mapping R -> R\nfeature_sample::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution: Current Sample from sampler\nfeature_parameters::Union{Nothing, Dict{S}} where S<:AbstractString: hyperparameters in Feature (and not in Sampler)\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.ScalarFourierFeature","page":"Features","title":"RandomFeatures.Features.ScalarFourierFeature","text":"ScalarFourierFeature(\n    n_features::Int64,\n    sampler::Sampler;\n    feature_parameters\n) -> ScalarFeature{String, Cosine}\n\n\nConstructor for a ScalarFeature with cosine features\n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.ScalarNeuronFeature","page":"Features","title":"RandomFeatures.Features.ScalarNeuronFeature","text":"ScalarNeuronFeature(\n    n_features::Int64,\n    sampler::Sampler;\n    activation_fun,\n    kwargs...\n) -> ScalarFeature{String, Relu}\n\n\nConstructor for a ScalarFeature with activation-function features (default ReLU)\n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.build_features-Tuple{ScalarFeature, AbstractMatrix, AbstractVector{Int64}}","page":"Features","title":"RandomFeatures.Features.build_features","text":"build_features(\n    rf::ScalarFeature,\n    inputs::AbstractMatrix,\n    batch_feature_idx::AbstractVector\n) -> Any\n\n\nbuilds features (possibly batched) from an input matrix of size (input dimension, number of samples) output of dimension (number of samples, 1, number features) \n\n\n\n\n\n","category":"method"},{"location":"API/Features/#vector-features","page":"Features","title":"Vector Features","text":"","category":"section"},{"location":"API/Features/","page":"Features","title":"Features","text":"VectorFeature\nVectorFourierFeature\nVectorNeuronFeature\nbuild_features(rf::VectorFeature,inputs::AbstractMatrix,atch_feature_idx::AbstractVector{Int})","category":"page"},{"location":"API/Features/#RandomFeatures.Features.VectorFeature","page":"Features","title":"RandomFeatures.Features.VectorFeature","text":"struct VectorFeature{S<:AbstractString, SF<:ScalarFunction} <: RandomFeature\n\nContains information to build and sample RandomFeatures mapping from N-D -> M-D\n\nn_features::Int64: Number of features\noutput_dim::Int64: Dimension of output\nfeature_sampler::Sampler: Sampler of the feature distribution\nscalar_function::ScalarFunction: ScalarFunction mapping R -> R\nfeature_sample::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution: Current Sample from sampler\nfeature_parameters::Union{Nothing, Dict{String}}: hyperparameters in Feature (and not in Sampler)\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.VectorFourierFeature","page":"Features","title":"RandomFeatures.Features.VectorFourierFeature","text":"VectorFourierFeature(\n    n_features::Int64,\n    output_dim::Int64,\n    sampler::Sampler;\n    feature_parameters\n) -> VectorFeature{String, Cosine}\n\n\nConstructor for a VectorFeature with cosine features\n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.VectorNeuronFeature","page":"Features","title":"RandomFeatures.Features.VectorNeuronFeature","text":"VectorNeuronFeature(\n    n_features::Int64,\n    output_dim::Int64,\n    sampler::Sampler;\n    activation_fun,\n    kwargs...\n) -> VectorFeature{String, Relu}\n\n\nConstructor for a VectorFeature with activation-function features (default ReLU)\n\n\n\n\n\n","category":"function"},{"location":"API/Features/#RandomFeatures.Features.build_features-Tuple{VectorFeature, AbstractMatrix, AbstractVector{Int64}}","page":"Features","title":"RandomFeatures.Features.build_features","text":"build_features(\n    rf::VectorFeature,\n    inputs::AbstractMatrix,\n    batch_feature_idx::AbstractVector\n) -> Any\n\n\nbuilds features (possibly batched) from an input matrix of size (input dimension,number of samples) output of dimension (number of samples, output dimension, number features) \n\n\n\n\n\n","category":"method"},{"location":"API/Features/#scalar-functions","page":"Features","title":"Scalar Functions","text":"","category":"section"},{"location":"API/Features/","page":"Features","title":"Features","text":"ScalarFunction\nScalarActivation\napply_scalar_function","category":"page"},{"location":"API/Features/#RandomFeatures.Features.ScalarFunction","page":"Features","title":"RandomFeatures.Features.ScalarFunction","text":"abstract type ScalarFunction\n\nType of a function mapping 1D -> 1D\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.ScalarActivation","page":"Features","title":"RandomFeatures.Features.ScalarActivation","text":"abstract type ScalarActivation <: ScalarFunction\n\nType of scalar activation functions\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.apply_scalar_function","page":"Features","title":"RandomFeatures.Features.apply_scalar_function","text":"apply_scalar_function(\n    sf::ScalarFunction,\n    r::AbstractArray\n) -> Any\n\n\napply the scalar function sf pointwise to vectors or matrices\n\n\n\n\n\n","category":"function"},{"location":"API/Features/","page":"Features","title":"Features","text":"    Cosine\n    Relu\n    Lrelu\n    Gelu\n    Elu\n    Selu\n    Heaviside\n    SmoothHeaviside\n    Sawtooth\n    Softplus\n    Tansig\n    Sigmoid","category":"page"},{"location":"API/Features/#RandomFeatures.Features.Cosine","page":"Features","title":"RandomFeatures.Features.Cosine","text":"struct Cosine <: ScalarFunction\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Relu","page":"Features","title":"RandomFeatures.Features.Relu","text":"struct Relu <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Lrelu","page":"Features","title":"RandomFeatures.Features.Lrelu","text":"struct Lrelu{FT<:AbstractFloat} <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Gelu","page":"Features","title":"RandomFeatures.Features.Gelu","text":"struct Gelu <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Elu","page":"Features","title":"RandomFeatures.Features.Elu","text":"struct Elu{FT<:AbstractFloat} <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Selu","page":"Features","title":"RandomFeatures.Features.Selu","text":"struct Selu{FT<:AbstractFloat} <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Heaviside","page":"Features","title":"RandomFeatures.Features.Heaviside","text":"struct Heaviside <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.SmoothHeaviside","page":"Features","title":"RandomFeatures.Features.SmoothHeaviside","text":"struct SmoothHeaviside{FT<:AbstractFloat} <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Sawtooth","page":"Features","title":"RandomFeatures.Features.Sawtooth","text":"struct Sawtooth <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Softplus","page":"Features","title":"RandomFeatures.Features.Softplus","text":"struct Softplus <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Tansig","page":"Features","title":"RandomFeatures.Features.Tansig","text":"struct Tansig <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"API/Features/#RandomFeatures.Features.Sigmoid","page":"Features","title":"RandomFeatures.Features.Sigmoid","text":"struct Sigmoid <: ScalarActivation\n\n\n\n\n\n","category":"type"},{"location":"setting_up_vector/#Setting-up-a-Vector-Random-Feature-Method","page":"Vector method","title":"Setting up a Vector Random Feature Method","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"A basic creation of vector-valued Fourier-based random feature method is given as:","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"# user inputs required:\n# paired input-output data - io_pairs::PairedDataContainer \n# parameter distribution   - pd::ParameterDistribution \n# number of features       - n_features::Int\n\nfeature_sampler = FeatureSampler(pd) \nfourier_vf = VectorFourierFeature(n_features, feature_sampler) \nrfm = RandomFeatureMethod(fourier_vf)\nfitted_features = fit(rfm, io_pairs)","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"Prediction at new inputs are made with","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"# user inputs required\n# new test inputs - i_test::DataContainer\n\npredicted_mean, predicted_cov = predict(rfm, fitted_features, i_test)","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"We see the core objects","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"ParameterDistribution: a flexible container for constructing constrained parameter distributions, (from EnsembleKalmanProcesses.jl)\n(Paired)DataContainer: consistent storage objects for input-output pairs or just inputs, (from EnsembleKalmanProcesses.jl)\nFeatureSampler: Builds the random feature distributions from a parameter distribution\nVectorFourierFeature: Special constructor of a Cosine-based VectorFeature from the random feature distributions\nRandomFeatureMethod: Sets up the learning problem (with e.g. batching, regularization)\nFit: Stores fitted features from the fit method","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"note: See some examples!\nRunning the test suite with TEST_PLOT_FLAG = true produces a 1-Dto p-D example produced by test/Methods/runtests.jl. These use realistic optional arguments and distributions.","category":"page"},{"location":"setting_up_vector/#ParameterDistributions","page":"Vector method","title":"ParameterDistributions","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The simplest construction of parameter distributions is by using the constrained_gaussian construction.","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"using RandomFeatures.ParameterDistributions","category":"page"},{"location":"setting_up_vector/#**Recommended**-univariate-and-product-distribution-build","page":"Vector method","title":"Recommended univariate and product distribution build","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The easiest constructors are for univariate and products","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"# constrained_gaussian(\"xi\", desired_mean, desired_std, lower_bound, upper_bound)\none_dim_pd = constrained_gaussian(\"xi\", 10, 5, -Inf, Inf) # Normal distribution\nfive_dim_pd = constrained_gaussian(\"xi\", 10, 5, 0, Inf, repeats = 5) # Log-normal (approx mean 10 & approx std 5) in each of the five dimensions","category":"page"},{"location":"setting_up_vector/#Simple-matrixvariate-distribution","page":"Vector method","title":"Simple matrixvariate distribution","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"Simple unconstrained distribution is created as follows. ","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"using Distributions\nd = 2\np = 5\nM = zeros(d,p)\nU = Diagonal(ones(d))\nV = SymTridiagonal(2 * ones(p), ones(p - 1))\n\ntwo_by_five_dim_pd = ParameterDistribution(\n    Dict(\"distribution\" => Parameterized(MatrixNormal(M, U, V)), # the distribution\n         \"constraint\" => repeat([no_constraint()], d * p), # flattened constraints \n         \"name\" => \"xi\",\n      ),\n)","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"note:  xi? \nThe name of the distribution of the features must be \"xi\"","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"note: Further distributions\nCombined distributions can be made using the VectorOfParameterized, or histogram-based distributions with Samples. Extensive documentation of distributions and constraints is found here.","category":"page"},{"location":"setting_up_vector/#Sampler","page":"Vector method","title":"Sampler","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The random feature distribution mathcalD is built of two distributions, the user-provided mathcalD_Xi (\"xi\") and a bias distribution (\"bias\"). The bias distribution is p-dimensional, and commonly uniformly distributed, so we provide an additional constructor for this case","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"theta = (XiB) sim mathcalD = (mathcalD_Xi mathcalU(c_ellc_u^p))","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"Defaults c_ell = 0 c_u = 2pi. In the code this is built as","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"sampler = FeatureSampler(\n    parameter_distribution,\n    output_dim;\n    uniform_shift_bounds = [0,2*π],\n    rng = Random.GLOBAL_RNG\n)","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"A random number generator can be provided. The second argument can be replaced with a general p-D ParameterDistribution with a name-field \"bias\".","category":"page"},{"location":"setting_up_vector/#Features:-VectorFeature-d-D-\\to-p-D","page":"Vector method","title":"Features: VectorFeature d-D to p-D","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"Given xinmathbbR^n input data, and m features, Features produces samples of","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"(Phi(xtheta_j))_ell = (sigma f(Xi_j x + B_j))_ellqquad theta_j=(Xi_jB_j) sim mathcalDqquad mathrmfor j=1dotsm  textand  ell=1dotsp","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"Note that Phi in mathbbR^nmp.  Choosing f as a cosine produces fourier features","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"vf = VectorFourierFeature(\n    n_features,\n    output_dim,\n    sampler;\n    kwargs...\n) ","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"f as a neuron activation produces a neuron feature (ScalarActivation listed here) ","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"vf = VectorNeuronFeature(\n    n_features,\n    output_dim,\n    sampler;\n    activation_fun = Relu(),\n    kwargs...\n) ","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The keyword feature_parameters = Dict(\"sigma\" => a), can be included to set the value of sigma.","category":"page"},{"location":"setting_up_vector/#Method","page":"Vector method","title":"Method","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The RandomFeatureMethod sets up the training problem to learn coefficients betainmathbbR^m from input-output training data (xy)=(x_iy_i)_i=1^n, y_i in mathbbR^p  and parameters theta = theta_j_j=1^m. Regularization is provided through Lambda = lambda otimes I_ntimes n from a user-provided p-by-p positive-definite regularization matrix lambda. In Einstein summation notation the method solves the following system","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"(frac1mPhi_nip(xtheta) Phi_njp(xtheta) + R_ij) beta_j = Phi(xtheta)_nipy_np","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"and where the regularization R is built from Lambda by solving the non-square system","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":" Phi_mjq R_ij = Phi_nip Lambda_pqnm","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"In the case the lambda is provided as a Real or UniformScaling then R is (consistently) replaced with lambda I_mtimes m. The authors typically recommend replacing non-diagonal lambda with mathrmdet(lambda)^frac1pI, which often provides a reasonable approximation, as there is additional computational expense through solving this un-batched system of equations.","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"note: The nonsquare system\nIf one chooses to solve for R, note that it is also only defined for dimensions m  np. For stability we also perform the following: we use a truncated SVD for when the rank of the system is less than m, and we also symmetrize the resulting matrix.","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"rfm = RandomFeatureMethod(\n    vf;\n    regularization = 1e12 * eps() * I,\n    batch_sizes = (\"train\" => 0, \"test\" => 0, \"feature\" => 0),\n)","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"One can select batch sizes to balance the space-time (memory-process) trade-off. when building and solving equations by setting values of \"train\", test data \"test\" and number of features \"feature\". The default is no batching (0).","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"warning: Conditioning\nThe problem is ill-conditioned without regularization. If you encounters a Singular or Positive-definite exceptions, try increasing the constant scaling regularization","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The solve for beta occurs in the fit method. In the Fit object we store the system matrix, its factorization, and the inverse of the factorization. For many applications this is most efficient representation, as predictions (particularly of the covariance) are then only a matrix multiplication.","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"fitted_features = fit(\n    rfm,\n    io_pairs; # (x,y)\n    decomposition = \"cholesky\",\n)","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The decomposition is based off the LinearAlgebra.Factorize functions. For performance we have implemented only (\"cholesky\" (default), \"svd\", and for Lambda=00 (not recommended) the method defaults to \"pinv\"). ","category":"page"},{"location":"setting_up_vector/#Hyperparameters","page":"Vector method","title":"Hyperparameters","text":"","category":"section"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"[Coming soon]","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"note: Note\nHyperparameter selection is very important for a good random feature fit.","category":"page"},{"location":"setting_up_vector/","page":"Vector method","title":"Vector method","text":"The hyperparameters are the parameters appearing in the random feature distribution mathcalD. We have an examples where an ensemble-based algorithm is used to optimize such parameters in examples/Learn_hyperparameters/","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Thank you for considering contributing to RandomFeatures! We encourage opening issues and pull requests (PRs).","category":"page"},{"location":"contributing/#What-to-contribute?","page":"Contributing","title":"What to contribute?","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The easiest way to contribute is by using RandomFeatures, identifying problems and opening issues;\nYou can try to tackle an existing issue. It is best to outline your proposed solution in the issue thread before implementing it in a PR;\nWrite an example or tutorial. It is likely that other users may find your use of RandomFeatures insightful;\nImprove documentation or comments if you found something hard to use;\nImplement a new feature if you need it. We strongly encourage opening an issue to make sure the administrators are on board before opening a PR with an unsolicited feature addition.","category":"page"},{"location":"contributing/#Using-git","page":"Contributing","title":"Using git","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you are unfamiliar with git and version control, the following guides will be helpful:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Atlassian (bitbucket) git tutorials. A set of tips and tricks for getting started with git.\nGitHub's git tutorials. A set of resources from GitHub to learn git.","category":"page"},{"location":"contributing/#Forks-and-branches","page":"Contributing","title":"Forks and branches","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Create your own fork of RandomFeatures on GitHub and check out your copy:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git clone https://github.com/<your-username>/RandomFeatures.jl.git\n$ cd RandomFeatures.jl","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Now you have access to your fork of RandomFeatures through origin. Create a branch for your feature; this will hold your contribution:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout -b <branchname>","category":"page"},{"location":"contributing/#Some-useful-tips","page":"Contributing","title":"Some useful tips","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When you start working on a new feature branch, make sure you start from main by running: git checkout main and git pull.\nCreate a new branch from main by using git checkout -b <branchname>.","category":"page"},{"location":"contributing/#Develop-your-feature","page":"Contributing","title":"Develop your feature","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Make sure you add tests for your code in test/ and appropriate documentation in the code and/or in docs/. Before committing your changes, you can verify their behavior by running the tests, the examples, and building the documentation locally. In addition, make sure your feature follows the formatting guidelines by running","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"julia --project=.dev .dev/climaformat.jl .","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"from the RandomFeatures.jl directory.","category":"page"},{"location":"contributing/#Squash-and-rebase","page":"Contributing","title":"Squash and rebase","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When your PR is ready for review, clean up your commit history by squashing and make sure your code is current with RandomFeatures.jl main by rebasing. The general rule is that a PR should contain a single commit with a descriptive message.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To make sure you are up to date with main, you can use the following workflow:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout main\n$ git pull\n$ git checkout <name_of_local_branch>\n$ git rebase main","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This may create conflicts with the local branch. The conflicted files will be outlined by git. To resolve conflicts, we have to manually edit the files (e.g. with vim). The conflicts will appear between >>>>, ===== and <<<<<. We need to delete these lines and pick what version we want to keep.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To squash your commits, you can use the following command:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~n","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"where n is the number of commits you need to squash into one. Then, follow the instructions in the terminal. For example, to squash 4 commits:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~4","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"will open the following file in (typically) vim:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   pick 6340aaa <commit message 2>\n   pick ebfd367 <commit message 3>\n   pick 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n##","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We want to keep the first commit and squash the last 3. We do so by changing the last three commits to squash and then do :wq on vim.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   squash 6340aaa <commit message 2>\n   squash ebfd367 <commit message 3>\n   squash 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Then in the next screen that appears, we can just delete all messages that we do not want to show in the commit. After this is done and we are back to  the console, we have to force push. We need to force push because we rewrote the local commit history.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git push -u origin <name_of_local_branch> --force","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can find more information about squashing here.","category":"page"},{"location":"contributing/#Unit-testing","page":"Contributing","title":"Unit testing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Currently a number of checks are run per commit for a given PR.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"JuliaFormatter checks if the PR is formatted with .dev/climaformat.jl.\nDocumentation rebuilds the documentation for the PR and checks if the docs are consistent and generate valid output.\nUnit Tests run subsets of the unit tests defined in tests/, using Pkg.test(). The tests are run in parallel to ensure that they finish in a reasonable time. The tests only run the latest commit for a PR, branch and will kill any stale jobs on push. These tests are only run on linux (Ubuntu LTS).","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Unit tests are run against every new commit for a given PR, the status of the unit-tests are not checked during the merge process but act as a sanity check for developers and reviewers. Depending on the content changed in the PR, some CI checks that are not necessary will be skipped.  For example doc only changes do not require the unit tests to be run.","category":"page"},{"location":"contributing/#The-merge-process","page":"Contributing","title":"The merge process","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We use bors to manage merging PR's in the the RandomFeatures repo. If you're a collaborator and have the necessary permissions, you can type bors try in a comment on a PR to have integration test suite run on that PR, or bors r+ to try and merge the code.  Bors ensures that all integration tests for a given PR always pass before merging into main. The integration tests currently run example cases in examples/. Any breaking changes will need to also update the examples/, else bors will fail.","category":"page"},{"location":"API/Methods/#Methods","page":"Methods","title":"Methods","text":"","category":"section"},{"location":"API/Methods/","page":"Methods","title":"Methods","text":"CurrentModule = RandomFeatures.Methods","category":"page"},{"location":"API/Methods/","page":"Methods","title":"Methods","text":"    RandomFeatureMethod\n    Fit\n    get_random_feature\n    get_batch_sizes\n    get_batch_size\n    get_regularization\n    sample(rfm::RandomFeatureMethod)\n    get_feature_factors\n    get_coeffs\n    fit\n    predict\n    predict!\n    predictive_mean\n    predictive_mean!\n    predictive_cov\n    predictive_cov!\n    predict_prior\n    predict_prior_mean\n    predict_prior_cov","category":"page"},{"location":"API/Methods/#RandomFeatures.Methods.RandomFeatureMethod","page":"Methods","title":"RandomFeatures.Methods.RandomFeatureMethod","text":"struct RandomFeatureMethod{S<:AbstractString, USorM<:Union{AbstractMatrix, LinearAlgebra.UniformScaling}}\n\nHolds configuration for the random feature fit\n\nrandom_feature::RandomFeature: The random feature object\nbatch_sizes::Dict{S, Int64} where S<:AbstractString: A dictionary specifying the batch sizes. Must contain \"train\", \"test\", and \"feature\" keys\nregularization::Union{AbstractMatrix, LinearAlgebra.UniformScaling}: A positive definite matrix used during the fit method to regularize the linear solve\ntullio_threading::Bool: Use multithreading provided by Tullio\n\n\n\n\n\n","category":"type"},{"location":"API/Methods/#RandomFeatures.Methods.Fit","page":"Methods","title":"RandomFeatures.Methods.Fit","text":"struct Fit{V<:(AbstractVector), USorM<:Union{AbstractMatrix, LinearAlgebra.UniformScaling}}\n\nHolds the coefficients and matrix decomposition that describe a set of fitted random features.\n\nfeature_factors::Decomposition: The LinearAlgreba matrix decomposition of (1 / m) * Feature^T * Feature + regularization\ncoeffs::AbstractVector: Coefficients of the fit to data\nregularization::Union{AbstractMatrix, LinearAlgebra.UniformScaling}: feature-space regularization used during fit\n\n\n\n\n\n","category":"type"},{"location":"API/Methods/#RandomFeatures.Methods.get_random_feature","page":"Methods","title":"RandomFeatures.Methods.get_random_feature","text":"get_random_feature(\n    rfm::RandomFeatureMethod\n) -> RandomFeature\n\n\ngets the random_feature field\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.get_batch_sizes","page":"Methods","title":"RandomFeatures.Methods.get_batch_sizes","text":"get_batch_sizes(\n    rfm::RandomFeatureMethod\n) -> Dict{S, Int64} where S<:AbstractString\n\n\ngets the batch_sizes field\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.get_batch_size","page":"Methods","title":"RandomFeatures.Methods.get_batch_size","text":"get_batch_size(\n    rfm::RandomFeatureMethod,\n    key::AbstractString\n) -> Int64\n\n\nget the specified batch size from batch_sizes field\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.get_regularization","page":"Methods","title":"RandomFeatures.Methods.get_regularization","text":"get_regularization(\n    rfm::RandomFeatureMethod\n) -> Union{AbstractMatrix, LinearAlgebra.UniformScaling}\n\n\ngets the regularization field\n\n\n\n\n\nget_regularization(\n    f::Fit\n) -> Union{AbstractMatrix, LinearAlgebra.UniformScaling}\n\n\ngets the regularization field (note this is the feature-space regularization)\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#StatsBase.sample-Tuple{RandomFeatureMethod}","page":"Methods","title":"StatsBase.sample","text":"sample(rfm::RandomFeatureMethod) -> Any\n\n\nsamples the random_feature field\n\n\n\n\n\n","category":"method"},{"location":"API/Methods/#RandomFeatures.Methods.get_feature_factors","page":"Methods","title":"RandomFeatures.Methods.get_feature_factors","text":"get_feature_factors(f::Fit) -> Decomposition\n\n\ngets the feature_factors field\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.get_coeffs","page":"Methods","title":"RandomFeatures.Methods.get_coeffs","text":"get_coeffs(f::Fit) -> AbstractVector\n\n\ngets the coeffs field\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#StatsAPI.fit","page":"Methods","title":"StatsAPI.fit","text":"fit(\n    rfm::RandomFeatureMethod,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer;\n    decomposition_type\n) -> Fit\n\n\nFits a RandomFeatureMethod to input-output data, optionally provide a preferred LinearAlgebra matrix decomposition. Returns a Fit object.\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#StatsAPI.predict","page":"Methods","title":"StatsAPI.predict","text":"predict(\n    rfm::RandomFeatureMethod,\n    fit::Fit,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer;\n    kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction of mean and (co)variance of fitted features on new input data\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#StatsAPI.predict!","page":"Methods","title":"StatsAPI.predict!","text":"predict!(\n    rfm::RandomFeatureMethod,\n    fit::Fit,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer,\n    mean_store::AbstractMatrix{<:AbstractFloat},\n    cov_store::AbstractArray{<:AbstractFloat, 3},\n    buffer::AbstractArray{<:AbstractFloat, 3};\n    kwargs...\n)\n\n\nMakes a prediction of mean and (co)variance of fitted features on new input data, overwriting the provided stores.\n\nmeanstore:`outputdimxn_samples`\ncovstore:`outputdimxoutputdimxnsamples`\nbuffer:n_samples x output_dim x n_features\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.predictive_mean","page":"Methods","title":"RandomFeatures.Methods.predictive_mean","text":"predictive_mean(\n    rfm::RandomFeatureMethod,\n    fit::Fit,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer;\n    kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction of mean of fitted features on new input data. Returns a output_dim x n_samples array.\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.predictive_mean!","page":"Methods","title":"RandomFeatures.Methods.predictive_mean!","text":"predictive_mean!(\n    rfm::RandomFeatureMethod,\n    fit::Fit,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer,\n    mean_store::Matrix{<:AbstractFloat};\n    kwargs...\n) -> Any\n\n\nMakes a prediction of mean of fitted features on new input data. Writes into a provided output_dim x n_samples array: mean_store.\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.predictive_cov","page":"Methods","title":"RandomFeatures.Methods.predictive_cov","text":"predictive_cov(\n    rfm::RandomFeatureMethod,\n    fit::Fit,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer,\n    prebuilt_features::AbstractArray{<:AbstractFloat, 3};\n    kwargs...\n) -> Any\n\n\nMakes a prediction of (co)variance of fitted features on new input data.\n\nReturns a output_dim x output_dim x n_samples array\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.predictive_cov!","page":"Methods","title":"RandomFeatures.Methods.predictive_cov!","text":"predictive_cov!(\n    rfm::RandomFeatureMethod,\n    fit::Fit,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer,\n    cov_store::AbstractArray{<:AbstractFloat, 3},\n    buffer::AbstractArray{<:AbstractFloat, 3},\n    prebuilt_features::AbstractArray{<:AbstractFloat, 3};\n    tullio_threading,\n    kwargs...\n)\n\n\nMakes a prediction of (co)variance of fitted features on new input data.\n\nWrites into a provided output_dim x output_dim x n_samples array: cov_store, and uses provided n_samples x output_dim x n_features buffer.\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.predict_prior","page":"Methods","title":"RandomFeatures.Methods.predict_prior","text":"predict_prior(\n    rfm::RandomFeatureMethod,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer;\n    kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction of mean and (co)variance with unfitted features on new input data\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.predict_prior_mean","page":"Methods","title":"RandomFeatures.Methods.predict_prior_mean","text":"predict_prior_mean(\n    rfm::RandomFeatureMethod,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer;\n    kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction of mean with unfitted features on new input data\n\n\n\n\n\n","category":"function"},{"location":"API/Methods/#RandomFeatures.Methods.predict_prior_cov","page":"Methods","title":"RandomFeatures.Methods.predict_prior_cov","text":"predict_prior_cov(\n    rfm::RandomFeatureMethod,\n    new_inputs::EnsembleKalmanProcesses.DataContainers.DataContainer;\n    kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction of (co)variance with unfitted features on new input data\n\n\n\n\n\n","category":"function"},{"location":"#RandomFeatures","page":"Home","title":"RandomFeatures","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A julia package to construct and apply random feature methods for regression. RandomFeatures can be viewed as an approximation of kernel methods. They can be used both as a substitution in Kernel ridge regression and Gaussian Process regresison. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nRandomFeatures Container of all tools\nSamplers Samplers for constrained probability distributions\nFeatures Builds feature functions from input data\nMethods Fits features to output data, and prediction on new inputs\nUtilities Utilities to aid batching, and matrix decompositions","category":"page"},{"location":"#Highlights","page":"Home","title":"Highlights","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A flexible probability distribution backend with which to sample features, with a comprehensive API\nA library of modular scalar functions to choose from\nMethods for solving ridge regression or Gaussian Process regression problem, with functions for producing predictive means and (co)variances using fitted features. \nExamples that demonstrate using the package EnsembleKalmanProcesses.jl to optimize hyperparameters of the probability distribution.","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RandomFeatures.jl is being developed by the Climate Modeling Alliance. The main developers are Oliver R. A. Dunbar and Thomas Jackson, with acknowledgement that the code was based on a python repository developed by Oliver R. A. Dunbar, Maya Mutic, and Nicholas H. Nelsen.","category":"page"},{"location":"installation_instructions/#Installation","page":"Installation instructions","title":"Installation","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"RandomFeatures.jl is a not a registered Julia package. To install perform the following in the julia command prompt","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia> ]\n(v1.8) pkg> add https://github.com/CliMA/RandomFeatures.jl\n(v1.8) pkg> instantiate","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"This will install the latest version of the package Git repository","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You can run the tests via the package manager by:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia> ]\n(v1.8) pkg> test RandomFeatures","category":"page"},{"location":"installation_instructions/#Cloning-the-repository","page":"Installation instructions","title":"Cloning the repository","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"If you are interested in getting your hands dirty and modifying the code then, you can also clone the repository and then instantiate, e.g.,","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> cd RandomFeatures.jl\n> julia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"info: Do I need to clone the repository?\nMost times, cloning the repository in not necessary. If you only want to use the package's functionality, adding the packages as a dependency on your project is enough.","category":"page"},{"location":"installation_instructions/#test-suite","page":"Installation instructions","title":"Running the test suite","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You can run the package's tests:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Alternatively, you can do this from within the repository:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project\njulia> ]\n(RandomFeatures) pkg> test","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"note: Plot outputs\nTests will output plots by setting the environment variable TEST_PLOT_FLAG. For example,julia> ENV[\"TEST_PLOT_FLAG\"] = true","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Once the project is built, you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project=docs/ -e 'using Pkg; Pkg.develop(PackageSpec(path=pwd())); Pkg.instantiate()'\n> julia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html","category":"page"},{"location":"installation_instructions/#Running-repository-examples","page":"Installation instructions","title":"Running repository examples","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"We have a selection of examples, found within the examples/ directory to demonstrate different use of our toolbox. Each example directory contains a Project.toml","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"To build with the latest RandomFeatures.jl release:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> cd examples/example-name/\n> julia --project -e 'using Pkg; Pkg.instantiate()'\n> julia --project example-file-name.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"If you wish to run a local modified version of RandomFeatures.jl then try the following (starting from the RandomFeatures.jl package root)","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> cd examples/example-name/\n> julia --project \n> julia> ]\n> (example-name)> rm RandomFeatures.jl\n> (example-name)> dev ../..\n> (example-name)> instantiate","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"followed by","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project example-file-name.jl","category":"page"}]
}
